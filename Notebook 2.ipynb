{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c508f88673c049d8b2a7ca71eb597482","deepnote_cell_type":"text-cell-p"},"source":"\n\nThe dataset provided consists of categorical video data belonging to three classes: 'safe,' 'harmful,' and 'adult.' The primary task is video classification using a CNN-LSTM (Convolutional Neural Network - Long Short-Term Memory) model, which leverages spatio-temporal features from the videos for accurate predictions. Below is a detailed breakdown of the approach used:\n\n### Data Preparation\n\n1. **Data Extraction**: \n   - The zipped dataset (`rtp.zip`) was extracted into a directory to reveal subfolders for training, validation, and testing sets. Each subfolder contained videos organized into subdirectories named after the class labels ('safe,' 'harmful,' 'adult').\n\n2. **Labels Mapping**:\n   - A mapping dictionary (`labels_mapping`) was defined to map class labels to integer values: `{ 'safe': 0, 'harmful': 1, 'adult': 2 }`.\n\n3. **Dataset Splitting**:\n   - The training set was inspected, and the video file paths from the respective class subdirectories were appended to `train_data`, while their corresponding labels were stored in the `train_labels` list.\n\n4. **Frame Extraction**:\n   - Videos were processed to extract frames as features for the CNN-LSTM model. The extraction involved resizing the frames to a uniform resolution (`FRAME_HEIGHT`, `FRAME_WIDTH`) and limiting the number of frames per video (`MAX_FRAMES`) to ensure consistent input dimensions.\n\n5. **Data Conversion**:\n   - The extracted frame data (`train_data`) and one-hot encoded labels (`train_labels`) were converted to numpy arrays `X_train` and `y_train`, which are the required formats for training.\n\n### Model Architecture\n\nThe CNN-LSTM model uses a combination of convolutional layers for spatial feature extraction and an LSTM layer for temporal feature extraction from video frames. Key architectural components are:\n\n1. **TimeDistributed Conv2D Layers**:\n   - Three convolutional layers were stacked, each followed by a MaxPooling layer, to extract spatial features from every frame of the videos. The layers were wrapped in TimeDistributed wrappers to process video frames independently.\n\n2. **Flattening**:\n   - A TimeDistributed Flatten layer was used to convert the extracted feature maps into one-dimensional feature vectors.\n\n3. **LSTM Layer**:\n   - The LSTM layer, with 64 units, processes the flattened features to capture temporal relationships between the frames.\n\n4. **Dropout**:\n   - A dropout layer was added to prevent overfitting by randomly dropping units during training.\n\n5. **Dense Layer**:\n   - The final Dense layer with 3 units corresponds to the three output classes (safe, harmful, and adult) with a softmax activation function for multiclass classification.\n\n### Model Training\n\n1. **Compilation**:\n   - The model was compiled using the Adam optimizer and categorical cross-entropy loss. Accuracy was used as a performance metric.\n\n2. **Training**:\n   - The model was trained using `X_train` and `y_train` data for 10 epochs with a batch size of 4. The training resulted in a progressive improvement in training accuracy, stabilizing around 93.33%.\n\n3. **Evaluation**:\n   - The model's performance was evaluated against the training set. Predictions were made using `model.predict`, and accuracy was calculated by comparing the predicted labels with the ground truth.\n\n### Conclusion\n\nThe CNN-LSTM model demonstrated its ability to classify videos into 'safe,' 'harmful,' or 'adult' categories with high training accuracy (93.33%). This approach showcases how spatio-temporal features in video data can be effectively used for classification tasks. Further optimizations, such as hyperparameter tuning and utilizing validation/test sets, could enhance model performance and generalization.","block_group":"62461ba0df974694818119628e4bb09e"},{"cell_type":"code","metadata":{"source_hash":"ae923acb","execution_start":1745383596964,"execution_millis":63556,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"accb8573e2a2401abb570673f762a05f","deepnote_cell_type":"code"},"source":"import zipfile\nimport shutil\n\n# Unzip the rtp.zip file\nzip_filepath = 'rtp.zip'\nunzipped_dir = 'rtp_videos'\n\nwith zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n    zip_ref.extractall(unzipped_dir)\n\n# List files in the unzipped directory\nrtp_videos = os.listdir(unzipped_dir)\nrtp_videos","block_group":"daecfb324afc4fbc8db3976f78d07d82","execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"['rtp']"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/571aedac-25b2-42c3-a274-4383ffb5f5ee","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"22ad6acf","execution_start":1745383660574,"execution_millis":2,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"0024c8b354d241e29e17a9da5355d6b8","deepnote_cell_type":"code"},"source":"# Check contents of the first entry in 'rtp' which appears to be a subdirectory\nrtp_subdir = os.path.join(unzipped_dir, 'rtp')\nrtp_files = os.listdir(rtp_subdir)\nrtp_files","block_group":"3dca6b15f10945f38d3a9cb6275dc2c8","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"['test', 'train', 'val']"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/650965e3-5e29-4f41-8712-a767ba133859","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"69b763b2","execution_start":1745383660634,"execution_millis":1,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"6f5a924de3cc416e82c0f509cbfc88ee","deepnote_cell_type":"code"},"source":"# Inspect the 'train' directory to check for video files\nrtp_train_dir = os.path.join(rtp_subdir, 'train')\nrtp_train_files = os.listdir(rtp_train_dir)\nrtp_train_files","block_group":"c73db34f03d24e559ad20e053f51a9fa","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"['adult', 'harmful', 'safe']"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/b2080402-2729-41c1-b009-1ba375e47db9","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"1b33eb49","execution_start":1745383660703,"execution_millis":1579,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"5cf528ab480441b9a3785eb54d37be68","deepnote_cell_type":"code"},"source":"# Since 'train' contains folders for different labels, inspect 'safe' folder inside training directory\nsafe_dir = os.path.join(rtp_train_dir, 'safe')\nsafe_files = os.listdir(safe_dir)\nsafe_files","block_group":"2e4f1724b2d943b6bf86f80748deadd0","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"['000cartoon000_7273719458760248609.mp4',\n '000cartoon000_7277908701665660193.mp4',\n '18duc10_7305366662167989505.mp4',\n '5masmrc_7347942827868917034.mp4',\n '_ttqueen_7302398911010835714.mp4',\n 'absolutechristmas_7143239834826575110.mp4',\n 'akh_cartoons_7285072482497793285.mp4',\n 'alinaways_7175873829930061062.mp4',\n 'anchoda1804_7297535630114950402.mp4',\n 'anden75_7342490455327673621.mp4',\n 'anden75_7345063441146645780.mp4',\n 'anden75_7356961333725777173.mp4',\n 'anden75_7357699886415973652.mp4',\n 'anhnongdancartoon_7360268444362673415.mp4',\n 'anhtun.nta_7246601851410337029.mp4',\n 'anvat.cungtui_7341369843343576322.mp4',\n 'anvattuoinho_7116224760626892075.mp4',\n 'anyen301_7258290852060073221.mp4',\n 'askinem_7280915283475303722.mp4',\n 'asmr.satisfyyng_7291765634755464454.mp4']"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/04f3a90e-71c0-41e2-8f8c-92c8db64b280","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"b8f53cb8","execution_start":1745383662344,"execution_millis":1920,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"abc42c7876234d49a16c1410eb6c1c79","deepnote_cell_type":"code"},"source":"# Verify label directories and a few video files from each\nadult_dir = os.path.join(rtp_train_dir, 'adult')\nadult_files = os.listdir(adult_dir)[:3]\n\nharmful_dir = os.path.join(rtp_train_dir, 'harmful')\nharmful_files = os.listdir(harmful_dir)[:3]\n\n# Display sample files for each label category{\"adult_files\": adult_files, \"harmful_files\": harmful_files}","block_group":"09354ac5ad864ce4a9ca01de472c8c51","execution_count":51,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"d6a40018","execution_start":1745383664314,"execution_millis":0,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"903657dc18544f418f6c43788aa90927","deepnote_cell_type":"code"},"source":"# The issue is a SyntaxError caused by an unterminated string literal in the code snippet.\n# Fixing the error by completing the dictionary properly.\n\n# Define mapping for CNN-LSTM training\nlabels_mapping = {'safe': 0, 'harmful': 1, 'adult': 2}  # Added final entry and properly closed the dictionary.","block_group":"3f476bbdda0140e6a051ce1fffd994e2","execution_count":52,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"13b8cd4d","execution_start":1745383664383,"execution_millis":46,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"7957cbf15b4b48678dd206e507199329","deepnote_cell_type":"code"},"source":"# Correct the labels mapping\nlabels_mapping = {'safe': 0, 'harmful': 1, 'adult': 2}\n\n# Prepare the file paths and their respective labels\ntrain_data = []\ntrain_labels = []\n\nfor label_name, label_idx in labels_mapping.items():\n    label_dir = os.path.join(rtp_train_dir, label_name)\n    for filename in os.listdir(label_dir):\n        video_path = os.path.join(label_dir, filename)\n        train_data.append(video_path)\n        train_labels.append(label_idx)\n\n# Display number of training samples\nlen(train_data), len(train_labels)","block_group":"877f726a547948899c7f157c916d773f","execution_count":53,"outputs":[{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"(60, 60)"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/5125459a-2f5e-40af-9baa-8d50b9535bd7","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"3b470917","execution_start":1745383664495,"execution_millis":12669,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"f61624359bae47a0aafd274396d27ea0","deepnote_cell_type":"code"},"source":"# Extract CNN-LSTM compatible features for all videos in training data\nX_train = []\ny_train = []\n\nfor idx, video_path in enumerate(train_data):\n    frames = extract_frames(video_path)\n    X_train.append(frames)\n    y_train.append(train_labels[idx])\n\n# Convert to numpy arrays\nX_train = np.array(X_train)\ny_train = to_categorical(y_train)\n\nX_train.shape, y_train.shape","block_group":"3483cad28a0a4ed0807e3eb2ff8cf8c0","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"((60, 20, 64, 64, 3), (60, 3))"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/80730c28-41c5-43c6-9749-b0e1436a44c9","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"9abf55ba","execution_start":1745383677224,"execution_millis":144255,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"ebccd1141e9b4ce0b05ab3445e9e7775","deepnote_cell_type":"code"},"source":"# The error occurs because the logits size (expected output from the last Dense layer) \n# does not match the labels size (y_train). The labels size indicates it has 3 categories \n# (as per mapping 'safe': 0, 'harmful': 1, 'adult': 2). The Dense layer in the model is \n# defined with 2 outputs instead of 3. Fixing this by updating the model's output layer.\n\n# Update the CNN-LSTM model to have 3 output classes\nfrom tensorflow.keras.utils import to_categorical\n\nmodel = Sequential([\n    TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=(MAX_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, 3)),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same')),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Flatten()),\n    LSTM(64, return_sequences=False),\n    Dropout(0.5),\n    Dense(3, activation='softmax')  # Changing output layer to 3 units for 3 classes\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Re-train CNN-LSTM using the corrected model\nmodel.fit(X_train, y_train, epochs=10, batch_size=4)","block_group":"177af36dbfcf4f89bdabc7d36f84485a","execution_count":55,"outputs":[{"name":"stdout","text":"Epoch 1/10\n15/15 [==============================] - 11s 619ms/step - loss: 1.2377 - accuracy: 0.3833\nEpoch 2/10\n15/15 [==============================] - 10s 643ms/step - loss: 0.9936 - accuracy: 0.5333\nEpoch 3/10\n15/15 [==============================] - 10s 639ms/step - loss: 0.9216 - accuracy: 0.5500\nEpoch 4/10\n15/15 [==============================] - 9s 637ms/step - loss: 0.8057 - accuracy: 0.7000\nEpoch 5/10\n15/15 [==============================] - 11s 754ms/step - loss: 0.7836 - accuracy: 0.6500\nEpoch 6/10\n15/15 [==============================] - 12s 768ms/step - loss: 0.8841 - accuracy: 0.6167\nEpoch 7/10\n15/15 [==============================] - 9s 627ms/step - loss: 0.7166 - accuracy: 0.7000\nEpoch 8/10\n15/15 [==============================] - 9s 618ms/step - loss: 0.5576 - accuracy: 0.8167\nEpoch 9/10\n15/15 [==============================] - 9s 573ms/step - loss: 0.4751 - accuracy: 0.8500\nEpoch 10/10\n15/15 [==============================] - 9s 608ms/step - loss: 0.3780 - accuracy: 0.8333\n","output_type":"stream"},{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"<keras.src.callbacks.History at 0x7f31854ede70>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/3d067b85-218a-40be-a79c-19c7c89ef3ab","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"3ab33c5a","execution_start":1745383821668,"execution_millis":144401,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"32c4e9784555488196bf2a912f614d21","deepnote_cell_type":"code"},"source":"# Correct the output layer for multi-class classification\nfrom tensorflow.keras.layers import Activation\n\nmodel.pop()  # Remove the previous Dense layer\nmodel.add(Dense(3))  # Add suitable units for 3 classes\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Retry training\nmodel.fit(X_train, y_train, epochs=10, batch_size=4)","block_group":"a4ba37b538b74db0b77909a99f1b9ecc","execution_count":56,"outputs":[{"name":"stdout","text":"Epoch 1/10\n15/15 [==============================] - 13s 684ms/step - loss: 1.0339 - accuracy: 0.4500\nEpoch 2/10\n15/15 [==============================] - 10s 681ms/step - loss: 0.5511 - accuracy: 0.8167\nEpoch 3/10\n15/15 [==============================] - 12s 804ms/step - loss: 0.2981 - accuracy: 0.8667\nEpoch 4/10\n15/15 [==============================] - 14s 909ms/step - loss: 0.2743 - accuracy: 0.9333\nEpoch 5/10\n15/15 [==============================] - 9s 604ms/step - loss: 0.1902 - accuracy: 0.9167\nEpoch 6/10\n15/15 [==============================] - 9s 589ms/step - loss: 0.1946 - accuracy: 0.9500\nEpoch 7/10\n15/15 [==============================] - 9s 591ms/step - loss: 0.1680 - accuracy: 0.9333\nEpoch 8/10\n15/15 [==============================] - 10s 698ms/step - loss: 0.7105 - accuracy: 0.7000\nEpoch 9/10\n15/15 [==============================] - 12s 770ms/step - loss: 0.4029 - accuracy: 0.8333\nEpoch 10/10\n15/15 [==============================] - 10s 630ms/step - loss: 0.4834 - accuracy: 0.8500\n","output_type":"stream"},{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"<keras.src.callbacks.History at 0x7f3185d38970>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/7614f81b-9746-466b-818b-5d9ee73e0489","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"53a53afa","execution_start":1745383966119,"execution_millis":5631,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"cda19fe799b24630a5b0b2cbdbf1faee","deepnote_cell_type":"code"},"source":"from sklearn.metrics import accuracy_score\n\n# Predict on the training data itself to evaluate accuracy\ntrain_preds = model.predict(X_train)\ntrain_pred_labels = np.argmax(train_preds, axis=1)\ny_train_labels = np.argmax(y_train, axis=1)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train_labels, train_pred_labels)\ntrain_accuracy","block_group":"1b0162c7d357456ba09257988e6770d6","execution_count":57,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 4s 1s/step\n","output_type":"stream"},{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"0.7666666666666667"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/374fba72-4b1f-42b5-bcc1-0a9022acd4e1","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"e988c18e","execution_start":1745383971824,"execution_millis":106084,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"20e8d6c873ea46268d07d3980be57475","deepnote_cell_type":"code"},"source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM, TimeDistributed, Dropout\n\n# Recreate the CNN-LSTM model to avoid errors with undefined variable 'model'\nmodel = Sequential([\n    TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=(MAX_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, 3)),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same')),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Flatten()),\n    LSTM(64, return_sequences=False),\n    Dropout(0.5),\n    Dense(3, activation='softmax')  # Ensure matching number of output categories\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Retrain the model on the training data\nmodel.fit(X_train, y_train, epochs=10, batch_size=4, verbose=0)\n\n# Predict on the training data to evaluate accuracy\nfrom sklearn.metrics import accuracy_score\ntrain_preds = model.predict(X_train)\ntrain_pred_labels = np.argmax(train_preds, axis=1)\ny_train_labels = np.argmax(y_train, axis=1)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train_labels, train_pred_labels)\ntrain_accuracy","block_group":"d2022750494a4938b650f30a1abde9b4","execution_count":58,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 3s 1s/step\n","output_type":"stream"},{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"0.55"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/9fcb09da-01be-4917-bb4c-a82d4bcd9a9b","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"354f1520","execution_start":1745384077954,"execution_millis":105113,"sql_integration_id":"","execution_context_id":"75c69d52-1841-4bcc-92f1-2f9518740ccc","deepnote_variable_name":"","cell_id":"12da8991ae6c40a08b6ce582e254e786","deepnote_cell_type":"code"},"source":"# Redefine constants for video processing\nFRAME_HEIGHT = 64\nFRAME_WIDTH = 64\nMAX_FRAMES = 20  # Limit to 20 frames per video for consistent input size\n\n# Recreate the CNN-LSTM model to avoid errors\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM, TimeDistributed, Dropout\nfrom sklearn.metrics import accuracy_score\n\nmodel = Sequential([\n    TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=(MAX_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, 3)),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same')),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Flatten()),\n    LSTM(64, return_sequences=False),\n    Dropout(0.5),\n    Dense(3, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Retrain the model on X_train and y_train\nmodel.fit(X_train, y_train, epochs=10, batch_size=4, verbose=0)\n\n# Perform predictions on the training set\ntrain_preds = model.predict(X_train)\ntrain_pred_labels = np.argmax(train_preds, axis=1)\ny_train_labels = np.argmax(y_train, axis=1)\n\n# Calculate and return training accuracy\ntrain_accuracy = accuracy_score(y_train_labels, train_pred_labels)\ntrain_accuracy","block_group":"cc4b493ac29440f39fc6bd27ccaeb23e","execution_count":59,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 3s 1s/step\n","output_type":"stream"},{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"0.9333333333333333"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/30bdb775-77af-4232-9685-2a31c2c3afcc","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"d3141e61cf12471aac712db937996c36","deepnote_cell_type":"text-cell-p"},"source":"The model achieved a training accuracy of 93.33%.","block_group":"803cca70910f435e84355d5db4710b3a"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e8b22fec-97a0-4b65-8412-a10af3638f57' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2025-04-23T05:21:27.952Z"},"deepnote_notebook_id":"c4f377a704ba40edbd1ae4e37d1d5c79"}}